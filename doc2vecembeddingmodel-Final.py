# -*- coding: utf-8 -*-
"""doc2vecEmbeddingModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mBnvnSP1lY2akG3wzdSeLJT-vP6srl8U
"""

saveModelPath = '/word2vecModel_Dblp'
dataPath = '/data.csv'
checkpoint_filepath = '/word2vecCheckpoint'

!pip install keras
!pip install numpy
!pip install pandas
!pip install sklearn

"""# New section"""

import pandas as pd
import random

df = pd.read_csv(dataPath)
df2 = df.sample(frac=1)
df2.head()

df.count()

dfDummy = df[(df['label'] == 1.0)]
dfDummy.head()

dfDummy.count()

import csv
from csv import DictReader
import random
import math

data_dblp = []
with open(dataPath, encoding="utf8", errors='ignore') as read_obj:
    dict_reader = DictReader(read_obj)
    data = list(dict_reader)
    for d in data:
        if d['label'] == '0' or d['label'] == '1':
            d['paperAbstract1'] = d['paperAbstract1'].strip()
            d['paperAbstract2'] = d['paperAbstract2'].strip()
            data_dblp.append(d)
        
print(len(data_dblp))
print(data_dblp[0])

random.shuffle(data_dblp)
train_data = data_dblp[0:math.floor(0.50* (len(data_dblp)))]
valid_data = data_dblp[math.floor(0.50* (len(data_dblp))):math.floor(0.75* (len(data_dblp)))]
test_data = data_dblp[math.floor(0.75 * (len(data_dblp))):len(data_dblp)]

print(len(train_data), "***************", len(test_data))



# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from keras.layers import Input, Embedding, Dot, Reshape, Dense
from keras.models import Model,load_model
import math
import keras

pairsData = []
source_mapping_id = {}

for data in train_data:
    pairsData.append(tuple((data['paperAbstract1'], data['paperAbstract2'], data['label'])))
    source_mapping_id[data['paperAbstract1']] = data['id1']

#print(pairsData)
#source_mapping_id

source_index = {}
source_index_inv = {}
#source_index = {data[0]: idx for idx, data in enumerate(pairsData, start = 0)}
m = 0

for idx in range(len(pairsData)):
    if not pairsData[idx][0] in source_index:
        source_index[pairsData[idx][0]] = m
        source_index_inv[m] = pairsData[idx][0]
        m = m + 1

print(len(source_index))

'''
temp = []
for k,v in source_index.items():
    temp.append(v)
temp.sort()
print(temp)
'''



## Checking
'''
strcheck = []
for idx, data in enumerate(pairsData):
    strcheck.append(data[1])
    
print(len(strcheck))
strcheck = set(strcheck)
print(len(strcheck))
'''

#print(len(pairsData))
target_index = {}
target_index_inv = {}
#target_index = {data[1]: idx for idx, data in enumerate(pairsData, start = 1)}

m = 0

for idx in range(len(pairsData)):
    if not pairsData[idx][1] in target_index:
        target_index[pairsData[idx][1]] = m
        target_index_inv[m] = pairsData[idx][1]
        m = m + 1
 
'''
temp = []
for k,v in target_index.items():
    temp.append(v)
temp.sort()
print(temp)
'''
#index_target = {idx: data for data, idx in target_index.items()}
#print(index_target[1])

finalData = []

for data in train_data:
    finalData.append(tuple((source_index[data['paperAbstract1']], target_index[data['paperAbstract2']], data['label'])))
    
#print(finalData)

## For generating negative samples
finalData_Unique = set(finalData)
print(len(finalData_Unique))

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

embedding_size = 100
def model():
    
    source = Input(name = 'source', shape = [1])
    target = Input(name = 'target', shape = [1])
    
    source_embedding = Embedding(name = 'source_embedding',input_dim = len(source_index),
                               output_dim = embedding_size)(source)
    
    target_embedding = Embedding(name = 'target_embedding',input_dim = len(target_index),
                               output_dim = embedding_size)(target)
    
    final_layer = Reshape([1])(Dot(name = 'dot_product', normalize = True, axes = 2)([source_embedding, target_embedding]))
    
    final_layer = Dense(1, activation = 'sigmoid')(final_layer)
    model = Model(inputs = [source, target], outputs = final_layer)
    opt = keras.optimizers.Adam(learning_rate=0.01)
    model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy',f1_m,precision_m, recall_m])

    return model

model = model()
model.summary()

import numpy as np
import random
random.seed(1000)

def batchifier(finalData, batch_size, positive_samples, negative_samples):
    batch = np.zeros((batch_size, 3))    
    while True:

        for idx, (source_id, target_id, label) in enumerate(random.sample(finalData, batch_size)):
            #if label != ' ':
            batch[idx, :] = (source_id, target_id, label)
        np.random.shuffle(batch)
        yield {'source': batch[:, 0], 'target': batch[:, 1]}, batch[:, 2]

batch_size = 1024
positive_samples = math.ceil(batch_size * 0.7)
negative_samples = math.floor(batch_size * 0.3)

print(positive_samples)
print(negative_samples)
gen = batchifier(finalData, batch_size, positive_samples, negative_samples)

# Generating Batch
next(batchifier(finalData, batch_size, positive_samples = 2, negative_samples = 2))

pairsData_v = []

for data in valid_data:
    pairsData_v.append(tuple((data['paperAbstract1'], data['paperAbstract2'], data['label'])))

source_index_v = {}
m = 0

for idx in range(len(pairsData_v)):
    if not pairsData_v[idx][0] in source_index_v:
        source_index_v[pairsData_v[idx][0]] = m
        m = m + 1

print(len(source_index_v))

target_index_v = {}
m = 0

for idx in range(len(pairsData_v)):
    if not pairsData_v[idx][1] in target_index_v:
        target_index_v[pairsData_v[idx][1]] = m
        m = m + 1
 
finalData_v = []

for data in valid_data:
    finalData_v.append(tuple((source_index_v[data['paperAbstract1']], target_index_v[data['paperAbstract2']], data['label'])))

# Train
'''
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_acc',
    mode='max',
    save_best_only=True)
'''
valid_data = batchifier(finalData_v, len(finalData_v), positive_samples, negative_samples)

# Steps_per_epoch = Training Size - too long, thus keeping it  - (TrainingSize // Batch_Size) 
h = model.fit(gen, epochs = 50,steps_per_epoch = len(finalData) // batch_size, verbose = 2)
#h = model.fit(gen, epochs = 50,steps_per_epoch = len(finalData) // batch_size, verbose = 2, validation_data= valid_data,callbacks=[model_checkpoint_callback])

model.save(saveModelPath)
#model.load_weights(checkpoint_filepath)



## Test
import pandas as pd
pairsDataTest = []
batch_size = 1024

#test_data = test_data[:1024]
for data in test_data:
    pairsDataTest.append(tuple((data['paperAbstract1'], data['paperAbstract2'], data['label'])))

source_index_test = {}
m = 0

for idx in range(len(pairsDataTest)):
    if not pairsDataTest[idx][0] in source_index_test:
        source_index_test[pairsDataTest[idx][0]] = m
        m = m + 1

target_index_test = {}
m = 0

for idx in range(len(pairsDataTest)):
    if not pairsDataTest[idx][1] in target_index_test:
        target_index_test[pairsDataTest[idx][1]] = m
        m = m + 1

finalDataTest = []

for data in test_data:
    finalDataTest.append(tuple((source_index_test[data['paperAbstract1']], target_index_test[data['paperAbstract2']], data['label'])))
    
#print(finalDataTest)
gen_test = batchifier(finalDataTest, batch_size, positive_samples = 2, negative_samples = 2)
x,y = next(batchifier(finalData, batch_size, positive_samples = 2, negative_samples = 2))

# Keeping Default Batch Size
predictions = model.predict(gen_test, verbose = 2, steps = len(test_data))
#print(predictions)
y_classes = predictions.argmax(axis=-1)
df = pd.DataFrame(y_classes)
print("=======Predictions=======")
print(df)

## Retriving the Score
score = model.evaluate(gen_test,verbose = 2, steps = len(test_data))
print(score)

df.head()
#df1 = df[df['0'] != 0]
#df1.head()

print(f"column types:\n{df.dtypes}")

#print(predictions)
y_classes = predictions.argmax(axis=-1)
df = pd.DataFrame(y_classes)
print("=======Predictions=======")
print(df)
print(predictions)
print(y_classes)

## Visualizations
from sklearn.manifold import TSNE

def extract_weights(name, model):
    """Extract weights from a neural network model"""
    
    # Extract weights
    weight_layer = model.get_layer(name)
    weights = weight_layer.get_weights()[0]
    
    # Normalize
    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))
    return weights

source_weights = extract_weights('source_embedding', model)

def reduceDimensions(weights, components = 3):
        return TSNE(components, metric = 'cosine').fit_transform(source_weights)

source_w = reduceDimensions(source_weights, components = 2)

source_w.shape

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'last'
import matplotlib.pyplot as plt
plt.figure(figsize = (10, 8))
plt.plot(source_w[:, 0], source_w[:, 1], 'r.')
plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('Papers Embeddings Visualized with TSNE');
plt.savefig("/Papers Embeddings Visualization.png")

target_weights = extract_weights('target_embedding', model)
target_w = reduceDimensions(target_weights, components = 2)

plt.figure(figsize = (10, 8))
plt.plot(target_w[:, 0], target_w[:, 1], 'b.')
plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('Cited Papers Embeddings Visualized with TSNE');

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('fivethirtyeight')
plt.rcParams['font.size'] = 15

def find_similar(name, weights, least = False):

    n = 10

    index = source_index
    rev_index = source_index_inv
    
    try:
        dists = np.dot(weights, weights[index[name]]) ## Cosine Similarity
    except KeyError:
        print(f'{name} Not Found.')
        return
  
    sorted_dists = np.argsort(dists)
    closest = sorted_dists[-n:]
    
    print(f'Paper IDs closest to {source_mapping_id[name]}.\n')

    for c in reversed(closest):
        print(f'{source_mapping_id[rev_index[c]]:} Similarity: {dists[c]:.{2}}')

find_similar('Regression testing is the verification that previously functioning software remains after a change. In this paper we report on a systematic review of empirical evaluations of regression test selection techniques published in major software engineering journals and conferences. Out of 2923 papers analyzed in this systematic review we identified 28 papers reporting on empirical comparative evaluations of regression test selection techniques. They report on 38 unique studies (23 experiments and 15 case studies) and in total 32 different techniques for regression test selection are evaluated. Our study concludes that no clear picture of the evaluated techniques can be provided based on existing empirical evidence except for a small group of related techniques. Instead we identified a need for more and better empirical studies were concepts are evaluated rather than small variations. It is also necessary to carefully consider the context in which studies are undertaken.', source_weights)

